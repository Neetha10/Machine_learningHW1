load('data3.mat');  
X = data(:, 1:end-1);  % except the last column
y = data(:, end);      % label column
step_size= 0.3;  % Step size
max_iter = 1000;       
X = [ones(size(X, 1), 1), X];  % adding a bias term to X
[samples, features] = size(X);
theta= randn(features, 1);
binary_error_tracker = zeros(max_iter, 1); 
perceptron_error_tracker = zeros(max_iter, 1);

iter=1;
errors=inf;
while iter <= max_iter && errors>0
    errors = 0;
    binary_error = 0;
    
    for i = 1:samples
       %Perceptron error
        if y(i) * (X(i, :) * theta) <= 0 
            theta = theta + step_size * y(i) * X(i, :)';
            errors = errors + 1;
        end
        
        % Binary classification error
        if sign(X(i, :) * theta) ~= y(i)
            binary_error = binary_error + 1;
        end
    end
    
    binary_error_tracker(iter) = binary_error / samples;
    perceptron_error_tracker(iter) = errors;
    
    
    if errors == 0
        fprintf('Converged after %d iterations\n', iter);
        break;
    end
    iter=iter+1;
end

% decision boundary
figure;
hold on;
gscatter(X(:, 2), X(:, 3), y, 'rb', 'xo');

x_vals = linspace(min(X(:, 2)) - 1, max(X(:, 2)) + 1, 100);
y_vals = -(theta(2) * x_vals + theta(1)) / theta(3);
plot(x_vals, y_vals, 'g-', 'LineWidth', 2);
xlabel('Feature 1');
ylabel('Feature 2');
title('Decision Boundary');
legend('Class 1', 'Class -1', 'Decision Boundary');
hold off;

figure;
subplot(1, 2, 1);
plot(binary_error_tracker(1:iter));
xlabel('Iteration');
ylabel('Binary Classification Error');
title('Over Time');

subplot(1, 2, 2);
plot(perceptron_error_tracker(1:iter));
xlabel('Iteration');
ylabel('perceptron error');
title('Over Time');
